<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Patrick Aschermayr | Welcome!</title>
    <link>https://paschermayr.github.io/author/patrick-aschermayr/</link>
      <atom:link href="https://paschermayr.github.io/author/patrick-aschermayr/index.xml" rel="self" type="application/rss+xml" />
    <description>Patrick Aschermayr</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 30 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://paschermayr.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Patrick Aschermayr</title>
      <link>https://paschermayr.github.io/author/patrick-aschermayr/</link>
    </image>
    
    <item>
      <title>Bayesian Inference in State Space Models - Part 3</title>
      <link>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-3/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-3/</guid>
      <description>&lt;p&gt;The MCMC in PMCMC - Making a Proposal Distribution for State Space Model Parameter&lt;/p&gt;
&lt;h1 id=&#34;almost-done&#34;&gt;Almost done!&lt;/h1&gt;
&lt;p&gt;In my previous blog posts

&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;
and 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;,
we explored a general framework on how to conduct parameter estimation
for state space models (SSM), and took a first step in implementing this machinery. In order to
apply Particle MCMC (PMCMC), we have to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Find a good proposal distribution $f(\theta^{\star} \mid \theta)$ for $ \theta^{\star}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Find a good proposal distribution $P(s^\star_{1:T} \mid \theta^{\star}, e_{1:T})$ for $ S^{\star}_{1:T}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Find a sufficiently &amp;lsquo;good&amp;rsquo; likelihood approximation $\hat{\mathcal{L}}$$_{\theta}(e_{1:T})$ that we can plug into the acceptance rate.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have already tackled the latter two problems in the previous posts, so what about the first one?&lt;/p&gt;
&lt;h1 id=&#34;the-mcmc-in-pmcmc&#34;&gt;The MCMC in PMCMC&lt;/h1&gt;
&lt;p&gt;If we want to jointly sample the parameter vector $\theta$, the first thing we need to do is to check the corresponding boundary conditions
of our parameter. For instance, if we want to estimate parameter of univariate normal observations, the mean parameter is
typically defined on the whole Reals, while the variance is constrained to be positive. There are several ways to proceed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;i. We could, in principle, use some multivariate proposal distribution $f( . \mid \theta)$, and reject all proposals where one of the parameter
falls outside of the corresponding boundaries. This is, as you already guessed, very wasteful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ii. Alternatively, you could use a truncated multivariate proposal distribution. Note that this is NOT the same as (i.), as one needs
to adjust the normalisation constants in the MH acceptance ratio, see

&lt;a href=&#34;https://darrenjw.wordpress.com/tag/truncate/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; for a more detailed explanation.
This approach is intuitively more efficient than the first option, but highly model specific and sometimes difficult to track for errors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;iii. Another possibility is to first transform the corresponding parameter into an unconstrained space, perform the MCMC step in this
unconstrained space, and then transform the proposed parameter back into the original space for the likelihood evaluation.
In this case, we have to be careful when calculating the prior. As we are now working with the transformed
parameter, we need to adjust the prior distribution with the Jacobian of the transform (keyword: push-forward measure). If you have trouble understanding that,
I recommend 
&lt;a href=&#34;https://stats.stackexchange.com/questions/174962/when-re-parametrizing-a-likelihood-function-is-it-enough-just-to-plug-in-the-tr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this explanation&lt;/a&gt;
before you proceed reading.
This approach has a much higher initial workload, but to my mind is the most general and effective way to perform
MCMC, and almost all libraries that you use work in this way under the hood. Consequently, we will focus on the third case.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, how can we code this up? The most straightforward way to implement this approach is by using information from the corresponding
prior: boundaries, length and dimension of each parameter can be inferred from here. I will use a new Julia package

&lt;a href=&#34;https://github.com/TuringLang/Bijectors.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bijectors.jl&lt;/a&gt; that makes the workflow significantly easier, but there are similar packages
in other languages as well.&lt;/p&gt;
&lt;h1 id=&#34;coding-it-all-up&#34;&gt;Coding it all up&lt;/h1&gt;
&lt;p&gt;In my 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last post&lt;/a&gt;, we implemented a particle
filter for basic hidden Markov models, so let us continue to use this example.
Hence, we are interested in the mean and variance parameter for each state, and the corresponding transition matrix of the
latent Markov chain. For now, let us assume the transition matrix is known, so we can implement
the MCMC machinery for scalar based parameter, and do not need to extend our functions for vector valued parameter. To my mind,
this would put too much attention to the code rather than the actual topic.
We start by creating a container that can hold one parameter and its corresponding priors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions, Bijectors, Parameters
import Base: count

#A container with necessary information about θ
mutable struct ParamInfo{T, D&amp;lt;:Union{Nothing, &amp;lt;:Distributions.Distribution} }
    value   :: T #Can be Real, Integer, or a Arrays composed of it
    prior   :: D #Corresponding prior of value, needs to fulfill boundary constraints!
end
ParamInfo(value::T) where {T} = ParamInfo(value, nothing)

#Check the number of parameter in ParamInfo struct
function count(paraminfo::ParamInfo)
    return length(paraminfo.value)
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;count (generic function with 17 methods)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I also defined a function that informs us about the parameter size inside this container. Next, we will make a
summary container for all the different parameter that we will need during the tutorial - i.e., mean and variance parameter of a univariate Normal distribution
for each state in the basic HMM case.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#A summary container for all relevant parameter
mutable struct HMMParameter
    μ   :: Vector{ParamInfo} #Observation Distribution Mean
    σ   :: Vector{ParamInfo} #Observation Distribution Variance
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Further, we want to define some helper functions that enable us to transform and inverse transform the parameter
in said container. Remember that our goal is to sample from some unconstrained space, and then inverse transform said parameter back to our constrained space. This will help us to accept more parameter proposals in the MCMC step. Note that I will use a subscript t for
certain variables in order to indicate that the operation takes place in the transformed space.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Wrapper to transform θ into an unconstrained space
function transform(parameter::ParamInfo)
    return Bijectors.link(parameter.prior, parameter.value)
end
#Wrapper to transform θₜ back into the original space, and store it in parameter struct
function inverse_transform!(parameter::ParamInfo, θₜ::T) where {T}
    value = Bijectors.invlink(parameter.prior, θₜ)
    @pack! parameter = value
    return value
end

#The same two functions dispatched on the summary container
function transform(parameter::HMMParameter)
    θₜ = Float64[]
    for field_name in fieldnames( typeof( parameter ) )
        sub_field = getfield(parameter, field_name )
        append!(θₜ, transform.(sub_field) )
    end
    return θₜ
end
function inverse_transform!(parameter::HMMParameter, θₜ::Vector{T}) where {T}
    counter = 1
    for field_name in fieldnames( typeof( parameter ) )
        sub_field = getfield(parameter, field_name )
        dim = sum( count.(sub_field) )
        inverse_transform!.( sub_field, θₜ[ counter:(-1 + counter + dim )] )
        counter += dim
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;inverse_transform! (generic function with 2 methods)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We must define this for both the individual ParamInfo as well as the summary container. This is, admittedly, not a straightforward process. However,
the important part is that we now have a container that holds all the
unknown observation parameter that we need in the HMM case, and we can freely transform and inverse transform parameter
that are contained in there. If you understand this goal, then you are perfectly fine to continue.&lt;/p&gt;
&lt;p&gt;As an example, let us now create an object that contains the mean and variance parameter of a univariate Normal two-state HMM. Next,
we transform all parameter into an unconstrained space, sample the transformed parameter and inverse transform them back into the original space:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Create a Vector of univariate Normal Model parameter and assign a prior to each of them:
μᵥ = [ParamInfo(-2.0, Normal() ), ParamInfo(2.0, Normal() )] #Initial value and Prior
σᵥ = [ParamInfo(1.0, Gamma() ), ParamInfo(1.0, Gamma(2,2) )] #Initial value and Prior

hmmparam = HMMParameter(μᵥ, σᵥ)

#Transform parameter:
transform(hmmparam)

#Sample parameter from an unconstrained distribution, then inverse transform and plug into our container:
θₜ_proposed = randn(4)
inverse_transform!(hmmparam, θₜ_proposed)
hmmparam #Check that parameter in hmmparam fulfill boundary conditions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well done, we achieved our initial goal! You can run the above code several times, and will see that the plugged in
parameter in the constrained space will satisfy the corresponding boundary conditions, even though we sampled from a multivariate
Normal distribution. At the very beginning, I mentioned that when transforming parameter in an unconstrained space,
one has to adjust the corresponding prior. These adjustments will all be handeled by the Bijectors package, so
we can conveniently write a function that calculates the prior for each of our parameter and returns the sum of it.
This will be helpful when we run the PMCMC algorithm in the next blog post.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#function to calculate prior including Jacobian adjustment
function calculate_logπₜ(parameter::ParamInfo)
    return logpdf_with_trans(parameter.prior, parameter.value, true)
end

#Wrapper to calculate prior including jacobian adjustment on all parameter
function calculate_logπₜ(parameter::HMMParameter)
    πₜ = 0.0
    for field_name in fieldnames( typeof( parameter ) )
        sub_field = getfield(parameter, field_name )
        πₜ += sum( calculate_logπₜ.(sub_field) )
    end
    return πₜ
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;calculate_logπₜ (generic function with 2 methods)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! Now we are almost done for today,
the last thing to do is to create a MCMC sampler. We will use a straightforward Metropolis algorithm with a multivariate Normal proposal distribution.
As before, we will first define a container with all the necessary information and a function on it to sample via this object:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Container with necessary information for a Metropolis MCMC step
mutable struct Metropolis{T&amp;lt;:Real}
    θₜ           ::  Vector{T}
    scaling     ::  Float64
end

# Function to propose a Metropolis step
function propose(sampler::Metropolis)
    @unpack θₜ, scaling = sampler
    return rand( MvNormal(θₜ, scaling) )
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;propose (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Scaling is a tuning parameter that determines how large the proposal steps for the parameter will be. The larger the tuning parameter,
the larger the proposed movements will be, which might result in a lower acceptance rate. Check this out yourself. At the end, we check our framework by assigning an initial parameter value
and sampling from there via this MCMC sampler:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Create an initial sampler, and propose new model parameter:
θₜ_inital = randn(4)
metropolissampler = Metropolis(θₜ_inital, 1/length(θₜ_inital) )
propose(metropolissampler)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;4-element Array{Float64,1}:
 -0.5791553185118626
  0.5875000886407763
 -0.09563827588507412
  1.1869926689230335
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;way-to-go&#34;&gt;Way to go!&lt;/h1&gt;
&lt;p&gt;This section was more straightforward than the previous one! We can now store, transform and inverse transform model parameter
of a basic HMM. We can also create a basic MCMC algorithm to sample said parameter. In my final post, we will combine the previous instalments of this series
and implement the PMCMC algorithm to estimate HMM model and state trajectory parameter jointly. See you soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inference in State Space Models - Part 2</title>
      <link>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-2/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-2/</guid>
      <description>&lt;p&gt;Particle Filtering in State Space Models&lt;/p&gt;
&lt;h1 id=&#34;welcome-back&#34;&gt;Welcome back!&lt;/h1&gt;
&lt;p&gt;In my 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous blog post&lt;/a&gt;, we explored a
general framework to conduct parameter estimation
on state space models (SSM). We call this framework Particle MCMC (PMCMC), and the three major
steps to successfully apply this algorithm are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;Find a good proposal distribution $f(\theta^{\star} \mid \theta)$ for $ \theta^{\star}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Find a good proposal distribution $P(s^\star_{1:T} \mid \theta^{\star}, e_{1:T})$ for $ S^{\star}_{1:T}$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Find a sufficiently &amp;lsquo;good&amp;rsquo; likelihood approximation $\hat{\mathcal{L}}$$_{\theta}(e_{1:T})$ that we can plug into the acceptance rate.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this post, we will try to solve problems 2. and 3. simultaneously. Hence, we need to find a way
to sample a state trajectory and get an approximation for the likelihood given the
current parameter vector $\theta$. In order to use all the code we use here, please load all the functions
from my 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory HMM&lt;/a&gt; article,
or just include the corresponding HMM Julia file from my 
&lt;a href=&#34;https://github.com/paschermayr/Shared-Code/blob/master/sampleHMM.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub account&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;filtering-the-latent-state-trajectory&#34;&gt;Filtering the latent state trajectory&lt;/h1&gt;
&lt;p&gt;In PMCMC, a particle filter (PF) is used to first sample a trajectory $s^{\star}_{1:T}$ and then calculate an
unbiased estimate of the corresponding likelihood. Note that unbiased does not mean our estimate is automatically
&amp;lsquo;good&amp;rsquo; enough, so there is quite a bit of ongoing research on how to keep the variance of particle filter likelihood estimates in check. For an in-depth explanation about
PFs, I refer to (Doucet and Johansen, 2011), as explaining the whole machinery of it in detail is simply out of reach in
a simple blog post.&lt;/p&gt;
&lt;p&gt;In a nutshell, particle filters are usually used to solve filtering equations in the form of
$$
\begin{equation}
\begin{split}
\pi_t( x_{1:t} ) &amp;amp;= \frac{ \tau_t(x_{1:t}) }{ Z_t }. \\&lt;br&gt;
&amp;amp;= \frac{ w_{1:t} q_t(x_{1:t}) }{ Z_t }\&lt;br&gt;
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Here, the goal is to sequentially sample a sequence of random variables, $X_t, t \in (1,&amp;hellip;, T)$ that come
from a sequence of target probabilities $\pi_t( x_{1:t} )$ with the same computational complexity at
each time step. The last part is crucial as we do not want to increase the computing time as more and more data comes in.
You will notice I rewrote the equation by introducing $w_{1:t}=\frac{ \tau_t(x_{1:t}) }{ q_t(x_{1:t}) }$. Those familiar with
importance sampling will likely understand why, but the reason we did so is because usually it is very difficult
to sample from $\tau_t(x_{1:t})$. We introduce an easier distribution $q_t(x_{1:t})$ and reweight this distribution via
$w_{1:t}$. Consequently, we need to find a good proposal distribution $q_t(x_{1:t})$, such that the computational complexity will not increase over time.
A convenient by-product is that we can approximate the normalizing constant
$Z_t$ with these weights:
$$
\begin{equation}
\hat{Z_t} = \frac{1}{t} \sum_{i=1}^{K} \frac{\tau_t(x_{1:t})}{q_t(x_{1:t})} = \frac{1}{t} \sum_{i=1}^{K} w_t(x^i_{1:t}).
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;In the basic HMM case, the joint distribution and the normalizing constant are of the form
$\tau_t(x_{1:t}) = \prod_{k=1}^{t} f(s_k \mid s_{k-1}) g(e_k \mid s_k)$
and $Z = p(e_{1:t})$. Hence you can use the weights from above to approximate the likelihood that we need in the PMCMC sampler. In order to have a constant
time complexity, one can use the Markovian properties of the latent states by imposing some transition density $q_{t}( x_{t} \mid x_{t-k})$
and propagating particles forward at each time step t. Afterwards,
these particles will be weighted with the weight function we introduced above. Particles that are very unlikely given the observed data
will then be replaced by more
realistic particles - this has the effect that variance of our likelihood estimate will not be &amp;lsquo;too&amp;rsquo; large. After we propagated
a bunch of particles forward up to the final time point T, we can sample one trajectory of these particles to get
$P(s^{\star}_{1:T} \mid \theta^{\star}, e_{1:T})$ for our PMCMC algorithm.&lt;/p&gt;
&lt;p&gt;If you read through the paper mentioned above, you will discover the optimal importance/proposal distribution
in terms of minimising the variance of the likelihood estimate for processes with
Markov property is $q(s_t \mid s_{t-1}, e_t) = p(s_t \mid s_{t-1}, e_t)$. This is usually not availabe, but gives some insight about good candidates.
A common and simple choice is the so-called Bootstrap filter,
which takes $q(s_t \mid s_{t-1}, e_t) = p(s_t \mid s_{t-1})$ and simplifies the corresponding particle weights to to $p(e_t \mid s_t)$.
Both of these distributions are readily available in the HMM and HSMM case, so we jump straight into the coding section.&lt;/p&gt;
&lt;h1 id=&#34;coding-our-very-first-particle-filter&#34;&gt;Coding our very first particle filter&lt;/h1&gt;
&lt;p&gt;The first thing that we need to define is the particle filter container itself. We will use the same style and structure as in our 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HMM post&lt;/a&gt;,
and the advantage of the Bootstrap filter is that we can just plug in the model distributions from the HMM to define our PF. As a result,
we define a field for the initial and transition distribution for our particles. The last field, observations, is used to weight the particles that we sample,
so we can filter our particles that do not seem to capture the data well enough:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Distributions, Parameters, Plots
import QuantEcon: gth_solve

mutable struct ParticleFilter
        initial         ::      Distributions.Distribution
        transition      ::      Vector{&amp;lt;:Distributions.Distribution}
        observations    ::      Vector{&amp;lt;:Distributions.Distribution}
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, we will define a helper function to calculate the corresponding weights in the log domain. This usually helps the numerical stability of the algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Helper function
function logsumexp(arr::AbstractArray{T}) where {T &amp;lt;: Real}
max_arr = maximum(arr)
max_arr + log(sum(exp.(arr .- max_arr)))
end

function logmeanexp(arr::AbstractVector{T}) where {T &amp;lt;: Real}
    log( 1/size(arr, 1) ) + logsumexp( arr )
end

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;logsumexp (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good! Now we can straight jump into the actual code. As always, I do my best to make as many comments as possible, but to fully understand each step,
there is probably no escape from reading the actual particle filter literature I mentioned above. Some comments to
understand the code a bit more clearly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The function input are the particle filter we defined above, and some observed data that we use to weight the particles that we propagate forward.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The function output is a log likelihood estimate, all the particle trajectories that we propagated forward, and a single trajectory that we will later on use in the PMCMC algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before we start the for loop, we pre-allocate the weights and the particles that we calculate later on. As you can see, we also normalize the weights, which I will denote with a
subscript norm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first particles are chosen from the initial distribution. If the latent states of the data are conceived as a subsequence of a
long-running process, the probability of the initial state should be set to the stationary state probabilities
of this unobserved Markov chain. Hence, this will be a function of the transition distribution of our model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The for-loop evolves around iteratively propagating particles forward and calculating
the corresponding weights of the particles. As you can see, we will not always reweighting the particles at each iteration, but only if a certain threshold is achieved.
This usually results in a even lower variance for our likelihood estimate. You can test this out yourself by altering the code and reweight at each iteration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;That&amp;rsquo;s it! Let us have a look:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Create a function to run a particle filter
function propose(pf::ParticleFilter, evidence::AbstractArray{T}; Nparticles=100, threshold=75) where {T&amp;lt;:Real}
        #Initialize variables
        @unpack initial, transition, observations = pf #Model parameter
        ℓlik = 0.0 #Log likelihood

        ℓweights = zeros(Float64, Nparticles)#Weights that are used to approximate log likelihood
        ℓweightsₙₒᵣₘ = similar(ℓweights)

        #initialize particles and sample first time point via initial distribution
        particles = zeros(Int64, size(evidence, 1), Nparticles )
        particles[1,:] =  rand(initial, Nparticles)

        #loop through time
        for t in 2:size(evidence, 1)

        #propagate particles forward
        particles[t, :] .= rand.( transition[ particles[t-1, :] ] )

        #Get new weights and calculate log likelihood
        ℓweights .= logpdf.( observations[ particles[t, :] ], evidence[t] )
        ℓweightsₙₒᵣₘ .= ℓweights .- logsumexp(ℓweights)
        ℓlik += logmeanexp(ℓweights) #add incremental likelihood

        #reweight particles if resampling threshold achieved
        if exp( -logsumexp(2. * ℓweightsₙₒᵣₘ) ) &amp;lt;= threshold
                paths = rand( Categorical( exp.(ℓweightsₙₒᵣₘ) ), Nparticles )
                particles .= particles[:, paths] #Whole trajectory!
        end

        end

        #Draw 1 trajectory path at the end
        path = rand( Categorical( exp.(ℓweightsₙₒᵣₘ) ) )
        trajectory = particles[:, path] #to keep type

        return ℓlik, particles, trajectory
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;propose (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! With the weights, we can obtain the likelihood approximation, and with the particles, we can get the
state trajectory that we need for the PMCMC algorithm. We still need to compute a function that translates our transition matrix to the
initial distribution of the corresponding Markov chain. As such, we create a helper function so we can extract all the transition matrix parameter from our particle filter:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Create a function so that we can obtain initial distribution from transition matrix
function get_transition_param(transitionᵥ::Vector{&amp;lt;:Categorical})
    return reduce(hcat, [transitionᵥ[iter].p for iter in eachindex(transitionᵥ)] )&#39;
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;get_transition_param (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do not worry, this function is not necessary to understand the mechanics of PMCMC. It is just a convenient wrapper
to create a transition matrix from all the parameter in our transition distribution. Now, let us check how good our algorithm works by first
sampling some HMM data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;T = 150
HMMevidence =  [Normal(2., .5), Normal(-2.,2.)]
HMMtransition = [ Categorical([0.95, 0.05]), Categorical([0.5, 0.5]) ]

state, observation = sampleHMM(HMMevidence, HMMtransition, T)
plot( layout=(2,1), label=false, margin=-2Plots.px)
plot!(observation, ylabel=&amp;quot;data&amp;quot;, label=false, subplot=1, color=&amp;quot;gold4&amp;quot;)
plot!(state, yticks = (1:2), ylabel=&amp;quot;state&amp;quot;, label=false, subplot=2, color=&amp;quot;black&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_6_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_6_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Next, let us create a Bootstrap particle filter with the corresponding transition and observation distribution from the HMM. If our code is correct, then
the particle trajectories of our PF should be very close to the latent state sequence from the sample HMM data. As already mentioned, the initial distribution will be
a function of the transition matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Initialize PF
pf = ParticleFilter( Categorical( gth_solve( Matrix(get_transition_param(HMMtransition) ) ) ),
                    HMMtransition,
                    HMMevidence
                    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool! Now let us run the particle filter once and plot the propagated trajectories against the sample data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;ll, particles, trajectory = propose(pf, observation; Nparticles=500, threshold=500 )

Plots.plot(state, label=&amp;quot;HMM latent states&amp;quot;, xlabel=&amp;quot;time&amp;quot;, ylabel=&amp;quot;latent state&amp;quot;)
Plots.plot!( mean(particles; dims=2) , label=&amp;quot;Particle Filter state trajectories&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_8_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_8_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Good! The propagated particle trajectories from our algorithm are close to the sample data, exactly what we wanted to have.
Play around with this yourself for different HMM parameter. There are, however, still a few aspects we have not yet dived into. For instance, how many particles do we
need to achieve a &amp;ldquo;good&amp;rdquo; likelihood approximation? In the basic HMM case, we could actually
calculate the likelihood exactly, and then check the solution against our approximation. This has been implemented many times, so it is easy to
code this up yourself. I will instead focus on a different aspect: we said that our approximation is unbiased, but what about the variance of this estimate?
There is a pretty good discussion and ideas on how to tune the number of particles on Professor Darren Wilkinson&amp;rsquo;s 
&lt;a href=&#34;https://darrenjw.wordpress.com/2014/06/08/tuning-particle-mcmc-algorithms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post about Particle MCMC&lt;/a&gt;.
We already know that we can decrease the variance of our estimate by inflating the number of particles, but this comes at the cost of additional computing time - so how much is enough? Moreover,
given we apply the particle filter in combination
with a MCMC algorithm, it is important to know on whether the variance of our estimate is constant across the support of our parameter. To check this, we
will initiate a grid of possible values for one parameter of your choice, and then run the particle filter several times
for each element in the grid. In the end, we can visually check if the variance for these estimates is (a) low
enough for your problem and (b) stays constant for a range of possible values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;#Check variance of likelihood estimate over a range of θ
function check_ll(pf::ParticleFilter, evidence::AbstractArray{T}, grid; runs = 20, Nparticles = 100) where {T&amp;lt;:Real}

        #Assign a matrix to store log likelihood estimate for each run
        ll_estimate = zeros(Float64, runs, length(grid))

        #Loop through the grid &amp;quot;runs&amp;quot; number of times, and assign the likelihood estimate to the preallocated matrix
        for iter in eachindex(grid)
                pf.observations[1] = Normal( grid[iter], pf.observations[1].σ)
                Base.Threads.@threads for idx in Base.OneTo(runs)
                        ll_estimate[idx, iter], _, _ = propose(pf, observation; Nparticles = Nparticles, threshold = Nparticles )
                end
        end

        #Return the log likelihood estimates
        return ll_estimate
end

grid = 1.0:0.05:3.0
ll_estimate = check_ll(pf, observation, grid; Nparticles = 500)
plot(grid, ll_estimate&#39;, seriestype = :scatter, ms=3.0, label=&amp;quot;&amp;quot;, xlabel=&amp;quot;Parameter value&amp;quot;, ylabel=&amp;quot;log likelihood estimate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_9_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_9_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In this case, the variance does not seem to increase drastically given enough particles. You can see the contrast when assigning only a fraction of them:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;ll_estimate = check_ll(pf, observation, grid; Nparticles = 50)
plot(grid, ll_estimate&#39;, seriestype = :scatter, ms=3.0, label=&amp;quot;&amp;quot;, xlabel=&amp;quot;Parameter value&amp;quot;, ylabel=&amp;quot;log likelihood estimate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_10_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/3%20Inference%20in%20State%20Space%20Models%20-%20Part%202_10_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In practice, you can choose the number of particles that you want to use by running a few trials for different parameter grids, but there are also methods to do so on the fly, some of them
mentioned in my linked blog post above.&lt;/p&gt;
&lt;h1 id=&#34;nice-one&#34;&gt;Nice one!&lt;/h1&gt;
&lt;p&gt;Wow, it took me a very long time to write this article, because it was extremely difficult to find a satisfying trade-off between making this
post intuitive and explaining enough theory to ensure the code above makes any sense to you. To be honest, I would recommend reading
one or two tutorials on particle filtering to fully understand the code, but I hope you could grasp the general goal through this blog post.
That being said, the most difficult part is done! We can now sample a state trajectory and obtain a likelihood estimate
for our MCMC algorithm for state space models. In part 3, we will use the most basic MCMC sampler and then
finish the PMCMC algorithm to obtain estimates for a basic HMM. See you soon!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Doucet, A. and Johansen, A. (2011). A tutorial on particle filtering and smoothing: Fifteen years later.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Inference in State Space Models - Part 1</title>
      <link>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-1/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://paschermayr.github.io/post/statespacemodels-3-inference-in-state-space-models-part-1/</guid>
      <description>&lt;p&gt;Inference in State Space Models - an Overview&lt;/p&gt;
&lt;h1 id=&#34;hi-there&#34;&gt;Hi there!&lt;/h1&gt;
&lt;p&gt;In my previous posts, I introduced two discrete state space model (SSM) variants: the

&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hidden Markov model&lt;/a&gt; and

&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-2-hsmm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hidden semi-Markov model&lt;/a&gt;. However, in all code examples,
model parameter were already given - what happens if we need to estimate them? This post is the first of a series of four blog entries, in which I will explain a general framework to perform parameter
inference on SSMs and implement a basic example for demonstration. Ideally, I would make all three posts as applied as my previous articles, but I have to use some theory in the
beginning to make things easier going forward.&lt;/p&gt;
&lt;p&gt;While SSMs are very flexible and can describe data with a complex structure, parameter estimation can be very challenging.
Analytical forms of the corresponding likelihood functions are only available in special cases
and, thus, standard parameter optimization routines might be unfeasible. Consequently, the major challenge in solving SSMs is the generally intractable
likelihood function $p_{\theta}(e_{1:t}) = \int p_{\theta}(e_{1:t}, s_{1:t}) d s_{1:t}$, which integrates over the (unknown) latent state trajectory. As previously mentioned,
$E_t$ is the observed data and $S_t$ the corresponding latent state.&lt;/p&gt;
&lt;h1 id=&#34;so-what-can-we-do&#34;&gt;So what can we do?&lt;/h1&gt;
&lt;p&gt;If both $E_t$ and $S_t$ follow a Normal distribution, one may analytically estimate the corresponding model parameter via the so called Kalman equations,
but given these very specific assumptions, we will not concentrate on this special case going forward.
If $S_{1:T}$ is discrete, then the most common point estimation technique is the EM-Algorithm.
In this case, one can use common filtering techniques, explained in (Rabiner, 1989), to iteratively calculate state probabilities and update model parameter.
Once the EM-Algorithm has reached some convergence criterion, one may estimate the most likely state trajectory given the estimated parameter. I will not go into
much detail here because this procedure has been explained on many occasions and will instead refer to the literature above.
However, both approaches do not work in many cases. The former technique only works with very specific model assumptions.
The latter technique is not available in case $S_{1:T}$ is continuous. Moreover, even if the state sequence is
discrete, summing out the state space might be computationally very challenging if the latent state structure is complex. In addition, one may not be able to
write down the analytical forms of the updating equations for more complex state space models.&lt;/p&gt;
&lt;p&gt;Another popular approach is to use Gibbs sampling, where one uses the previously mentioned filtering techniques to obtain a sample from the
whole state trajectory $S_{1:T}$, and then sample model parameter conditional on this path.
This technique suffers from the usually high autocorrelation within the model parameter vector and the state sequence.
Moreover, ideally one would like to perform estimation jointly for the model parameter $\theta$ and the latent state space $S_{1:T}$, as both have a high interdependence as well. This is not feasible for either method that I mentioned so far.
For a more in-depth discussion, an excellent comparison of point estimation and Bayesian techniques is given by (Ryden, 2008).&lt;/p&gt;
&lt;h1 id=&#34;a-general-framework-to-perform-inference-on-state-space-models&#34;&gt;A general framework to perform inference on state space models&lt;/h1&gt;
&lt;p&gt;Given the statements above, I will now focus on a general approach to perform joint estimation on the model parameter and state trajectory of state space models,
independent of the variate form of the latent space and the distributional assumptions on the observed variables. This is, to the best of my
knowledge, only possible in a Bayesian setting, where we target the full joint posterior $P( s_{1:T}, \theta \mid e_{1:T})$ by iterating over the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;propose $\theta^{\star} \sim f(\theta^{\star} \mid \theta)$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;propose $S^\star_{1:T} \sim P(s^{\star}_{1:T} \mid \theta^{\star}, e_{1:T})$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;and then accept this pair $(\theta^{\star}, s^{\star}_{1:T})$ with acceptance probability&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!---	$ \frac{P(E \mid \theta^{\star})}{P(E \mid \theta)} \frac{P(\theta^{\star})}{P(\theta)} \frac{q(\theta \mid \theta^{\star})}{q(\theta^{\star} \mid \theta)}$  --&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
A_{PMCMC} &amp;amp;= \frac{ P( s^{\star}_{1:T} \mid \theta^{\star}) }{  P(s_{1:T} \mid \theta ) }
\frac{ P(e_{1:T} \mid s^{\star}_{1:T}, \theta^{\star}) }{  P(e_{1:T} \mid s_{1:T}, \theta ) }
\frac{ P(s_{1:T} \mid e_{1:T}, \theta) }{  P(s^{\star}_{1:T} \mid e_{1:T}, \theta^{\star}) }
\frac{P(\theta^{\star})}{P(\theta)} \frac{q(\theta \mid \theta^{\star})}{q(\theta^{\star} \mid \theta)} \\&lt;br&gt;
&amp;amp;= \frac{P(e_{1:T} \mid \theta^{\star})}{P(e_{1:T} \mid \theta)} \frac{P(\theta^{\star})}{P(\theta)} \frac{q(\theta \mid \theta^{\star})}{q(\theta^{\star} \mid \theta)}. \&lt;br&gt;
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Okay, but what does that even mean? In the first step, we propose a new parameter vector $\theta^{\star}$ from a MCMC proposal distribution. I will not go over the basics
of MCMC, as this has been explained in many other articles, but we will implement an example in part 3 of this tutorial. Given this $\theta^{\star}$, we sample a new trajectory
$s^\star_{1:T}$ and jointly accept this pair with the acceptance ratio from point 3. This framework allows to jointly sample $\theta$ and $S_{1:T}$, which was our goal in the first place.
However, the in general intractable likelihood term is still contained in the acceptance probability in step 3,
which is simplified by using the basic marginal likelihood identity (BMI) from (Chib, 1995).
Consequently, the difficulty to obtain a sample from the posterior remains the same: we need to evaluate the (marginal) likelihood of the model.&lt;/p&gt;
&lt;h1 id=&#34;the-particle-mcmc-idea&#34;&gt;The Particle MCMC idea&lt;/h1&gt;
&lt;p&gt;An alternative is to replace this (marginal) likelihood with an unbiased estimate $\hat{ \mathcal{L} }$$_\theta(e_{1:T})$. In this setting, (Andrieu and Roberts, 2010) have shown the puzzling result that one
can do so and still target the exact posterior distribution of interest, thereby opening a completely new research area now called &amp;lsquo;exact approximate MCMC&amp;rsquo;.
The algorithm that is used in this setting targets the full posterior and is known as Particle MCMC (PMCMC).
Here, a particle filter (PF) is used to obtain a sample for $S_{1:T}$. A by-product of this procedure is that we also obtain an unbiased estimate for the likelihood.
If you are more interested in PFs, have a look at the paper from (Doucet, A. and Johansen, 2011).
For anyone wondering: as $p(s_{1:T} \mid e_{1:T}, \theta )$ is replaced with an approximation $\hat{p}(s_{1:T} \mid e_{1:T}, \theta )$,
the approximation does not admit $p(s_{1:T}, \theta \mid e_{1:T})$ as invariant density, but
this is corrected in the PMCMC algorithm via step 3.&lt;/p&gt;
&lt;h1 id=&#34;going-forward&#34;&gt;Going forward&lt;/h1&gt;
&lt;p&gt;The PMCMC machinery provides a very powerful framework and cures many of the difficulties in the estimation paradigms mentioned at the
beginning of this series. In reality, however, much of the performance depends on whether you find a good proposal
distribution $ f( . \mid \theta)$ and $P(s^\star_{1:T} \mid \theta^{\star}, e_{1:T})$.
Moreover, we have yet not mentioned how this machinery scales in the dimension of $\theta$ and $S_{1:T}$.
These are all questions that I pursue in my own PhD studies, and we may venture through them once we know the very basics.&lt;/p&gt;
&lt;p&gt;In the next blog post, we will implement a particle filter to do inference on the latent state trajectory. Going forward, we then talk about doing MCMC in this case,
and implement a basic framework to sample from an unconstrained parameter space. Last but not least, we will combine all of this and apply a PMCMC
algorithm on a standard HMM to obtain parameter estimates.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;Andrieu, C. and Roberts, G. O. (2010). The pseudo-marginal approach for efficient monte carlo computations.
Ann. Statist., 37(2):697-725.&lt;/p&gt;
&lt;p&gt;Chib, S. (1995). Marginal likelihood from the gibbs output. Journal of the American Statistical Association,
90(432):1313-1321.&lt;/p&gt;
&lt;p&gt;Doucet, A. and Johansen, A. (2011). A tutorial on particle filtering and smoothing: Fifteen years later.&lt;/p&gt;
&lt;p&gt;Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257-286.&lt;/p&gt;
&lt;p&gt;Ryden, T. (2008). Em versus markov chain monte carlo for estimation of hidden markov models: A computational
perspective. Bayesian Analysis, 3(4):659-688.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>State Space Models Everywhere! Round 1: HSMM</title>
      <link>https://paschermayr.github.io/post/statespacemodels-2-hsmm/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://paschermayr.github.io/post/statespacemodels-2-hsmm/</guid>
      <description>&lt;p&gt;Introducing Hidden semi-Markov Models&lt;/p&gt;
&lt;h1 id=&#34;hi-there&#34;&gt;Hi there!&lt;/h1&gt;
&lt;p&gt;In the 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last article&lt;/a&gt;, you got a first impression about state space models and, as an example,
the basic hidden Markov model:
&lt;img src=&#34;https://paschermayr.github.io/files/post/BayesianHMM.png&#34; alt=&#34;A plot&#34;&gt;&lt;/p&gt;
&lt;p&gt;We already talked about various advantages of state space models, but - depending on the features of the underlying data that you want to model -
basic HMMs just might not be good enough. Here is why:&lt;/p&gt;
&lt;h1 id=&#34;a-geometric-duration-distribution-is-a-problem&#34;&gt;A geometric duration distribution is a problem&lt;/h1&gt;
&lt;p&gt;Let us focus on the unobserved process, $S_t$, for now. We are interested in the actual time
spent in a particular state. Let us calculate the probability  that we are currently
in state $i$ and remain here for the next two time steps. For a discrete 2-state, homogenous Markov chain, using the chain rule and the Markov assumption of the basic model, we can write:
$$
\begin{equation}
\begin{split}
P( S_{t+3} = j, S_{t+2} = i, S_{t+1} = i \mid S_{t} = i ) &amp;amp;= P( S_{t+3} = j\mid S_{t+2} = i) P(S_{t+2} = i,  \mid S_{t+1} = i) P( S_{t+1} = i \mid S_{t} = i ) \\&lt;br&gt;
&amp;amp;= (1 - \tau_{ii}) * \tau_{ii}^2
\end{split}
\label{eq:HMM_geom1}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;In general, for $t+k$ steps:
$$
\begin{equation}
\begin{split}
P( S_{t+k} = j, \dots, S_{t+1} = i \mid S_{t} = i ) &amp;amp;= (1 - \tau_{ii}) * \tau_{ii}^{k-1} \\&lt;br&gt;
&amp;amp;= Geometric_{ \tau_{ii} },
\end{split}
\label{eq:HMM_geom2}
\end{equation}
$$
where the geometric distribution has to be interpreted as the length of state duration up to and including the transition to the other state.
Why is this a problem?
The hidden states you model may switch more rapidly than you would like them to
do. Even if you assign parameter very close to the boundaries of its support, the duration distribution will always implicitly be geometric.
That makes it unsuitable if you want to model something that is supposed to stay in a particular state for a long time. As an example, let us say you want to model economic cycles
of a developed country. You typically expect an expansion to last on average 5-10 years, but modelling such long durations is unfeasable with daily input data for an HMM. To circumvent this issue, one may
use weekly or monthly data, but why give up data and essential information if alternatives are available?&lt;/p&gt;
&lt;h1 id=&#34;from-hmms-to-hsmms&#34;&gt;From HMMs to HSMMs&lt;/h1&gt;
&lt;p&gt;As a solution, we can model these state duration probabilities explicitly. Such models are known as hidden semi-Markov models (HSMM), and they are a powerful
generalization of the basic HMM. HSMMs have an additional latent variable, lets call it $D_t$ for duration that
determines how long one may stay in any given state. $S_t$ will only have the Markov property while transitioning,
otherwise it is determined by $D_t$. The tuple $\{ S_t, D_t \}$ forms a so-called semi-Markov chain.
Let us visualize this:
&lt;img src=&#34;https://paschermayr.github.io/files/post/BayesianHSMM.png&#34; alt=&#34;A plot&#34;&gt;&lt;/p&gt;
&lt;p&gt;In an HSMM, transitions are allowed only at the end of each state, resulting in the following distributional forms:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
S_t \mid s_{t-1}, d_{t-1} \sim P( S_t \mid s_{t-1}, d_{t-1} ) = \begin{cases}
\delta( S_{t} = s_{t-1}) &amp;amp;\text{ $d_{t-1} &amp;gt; 0$ }\\&lt;br&gt;
\mathcal{T}_{s_{t-1},.} &amp;amp;\text{ $d_{t-1} = 0$ }
\end{cases}
\label{eq:EDHMM_transition}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
D_t \mid s_{t}, d_{t-1} \sim P(D_t \mid s_{t}, d_{t-1}) = \begin{cases}
\delta( D_{t} = d_{t-1} - 1) &amp;amp;\text{ $d_{t-1} &amp;gt; 0$ } \\&lt;br&gt;
\mathcal{D}_{s_{t}} &amp;amp;\text{ $d_{t-1} = 0$ }
\end{cases}
\label{eq:EDHMM_duration}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
E_t \mid s_{t} \sim \mathcal{O}_{s_{t}}
\label{eq:EDHMM_observation}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\delta(a,b)$ is the Kronecker product and equals $1$ if $a = b$ and $0$ otherwise.&lt;/p&gt;
&lt;p&gt;Note that, as we model the duration explicitly now, we have to slightly adjust the
transition matrix that we used in the 
&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous article&lt;/a&gt;. The diagonal elements of the transition matrix - the probability to remain in any given state in the next time step,
which formally is depicted as $\tau_{i,i} = P(s_{t+1} = i \mid s_{t} = i)$ - are now set to 0. The rest of the row elements in the transition matrix still need to sum up to 1. If you have trouble
understanding that, check out the code below.&lt;/p&gt;
&lt;h1 id=&#34;let-us-code&#34;&gt;Let us code!&lt;/h1&gt;
&lt;p&gt;To understand the code for sampling a single trajectory of said HSMM more clearly, keep reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The function input are the model distributions stated above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The function output is a single trajectory of the observed and latent variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before we start the for-loop over time, we need to define the initial state. If the latent states of the data are conceived as a subsequence of a
long-running process, the probability of the initial state should be set to the stationary state probabilities
of this unobserved Markov chain. This plays an important part in the estimation paradigm, but for now we simply choose any
of the available states with equal probability. Don&amp;rsquo;t worry if this sounds difficult - we will come back to it in a later article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The for-loop samples the new state given the old state, and then the observation given the new state, over time. The corresponding distributions
are stated above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the HSMM case, we check if the duration in the previous time step reached 0. If true, we sample a new state and duration given this state. If not,
the current state continues, and we decrease the duration count by 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;That&amp;rsquo;s it! Let us have a look:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;
using Distributions

function sampleHSMM(evidence::Vector{&amp;lt;:Distribution}, duration::Vector{&amp;lt;:Distribution}, transition::Matrix{Float64}, T::Int64)
        #Initialize states and observations
        state = zeros(Int64, T)
        state_length = zeros(Int64, T)
        observation = zeros(Float64, T)

        #Sample initial s from initial distribution
        state[1] = rand( 1:size(transition, 1) ) #not further discussed here
        state_length[1] = rand( duration[ state[1] ] ) #not further discussed here
        observation[1] = rand( evidence[ state[1] ] )

        #Loop over Time Index
        for time in 2:T
                if state_length[time-1] &amp;gt; 0
                        state[time] = state[time-1]
                        state_length[time] = state_length[time-1] - 1
                        observation[time] = rand( evidence[ state[time] ] )
                else
                        state[time] = rand( Categorical( transition[ state[time-1], :] ) )
                        state_length[time] = rand( duration[ state[time] ] )
                        observation[time] = rand( evidence[ state[time] ] )
                end
        end
        #Return output
        return state, state_length, observation
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;sampleHSMM (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us generate one sample path of said model. I am still using normal observation distributions, but added a third state  so you can have a visual example from the transition matrix adjustment talk above.
I will use a Negative Binomial distribution to model the state duration, but you are free to choose whatever you like best as long as the support lies on the non-negative Integers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;
using Plots

T = 5000
evidence =  [Normal(0., .5), Normal(0.,1.), Normal(0.,2.)]
duration =  [NegativeBinomial(100., .2), NegativeBinomial(10., .05), NegativeBinomial(50.,0.5)]
transition = [0.0 0.5 0.5;
              0.8 0.0 0.2;
              0.8 0.2 0.0;]
state, state_length, observation = sampleHSMM(evidence, duration, transition, T)

plot( layout=(3,1), label=false, margin=-2Plots.px)
plot!(observation, ylabel=&amp;quot;data&amp;quot;, label=false, subplot=1, color=&amp;quot;gold4&amp;quot;)
plot!(state, yticks = (1:3), ylabel=&amp;quot;state&amp;quot;, label=false, subplot=2, color=&amp;quot;black&amp;quot;)
plot!(state_length, ylabel=&amp;quot;duration&amp;quot;, label=false, subplot=3, color=&amp;quot;blue&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/2%20HSMM_2_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/2%20HSMM_2_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;To see why the HSMM is a large improvement over the previous model, try to mimic the average state duration with the code that we used for the

&lt;a href=&#34;https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;basic HMM model&lt;/a&gt;. Likewise, you can choose a geometric state duration in the example here to mimic the HMM case. While the latter is done easily,
the former should almost be unfeasable. As always, you can download the full script from my 
&lt;a href=&#34;https://github.com/paschermayr/Shared-Code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub account&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;thats-it-for-today&#34;&gt;That&amp;rsquo;s it for today!&lt;/h1&gt;
&lt;p&gt;Well done! Initially, I wanted to write an article about several model extensions, but I quickly figured that this would be much
too long for what I was planning to do, and therefore focused on HSMMs only in this post.
I am going to gradually write follow-up posts on this one, as there exist many more state space models with an interesting structure, such as autoregressive or factorial HMMs.
See you soon!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Primer on State Space Models</title>
      <link>https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/</link>
      <pubDate>Sun, 06 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://paschermayr.github.io/post/statespacemodels-1-a-primer-on-state-space-models/</guid>
      <description>&lt;p&gt;Introducing State Space Models&lt;/p&gt;
&lt;h1 id=&#34;welcome&#34;&gt;Welcome!&lt;/h1&gt;
&lt;p&gt;In my first series of posts, I will give a primer on state space models (SSM) that will lay a foundation in
understanding upcoming posts about their variants, usefulness, methods to apply inference and forecasting possibilities.
When talking about a state space model (SSM), people usually refer to a bivariate stochastic process $\{ E_t, S_t \}_{t = 1,2,\ldots ,T }$, where $S_t$ is an unobserved
Markov chain and $E_t$ is an observed sequence of random variables. This may sounds difficult now, so let us look at a graphical example of one of the
most well known SSMs out there - the so called Hidden Markov Model (HMM):
&lt;img src=&#34;https://paschermayr.github.io/files/post/BayesianHMM.png&#34; alt=&#34;A plot&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;so-what-are-ssms-really&#34;&gt;So, what are SSMs really?&lt;/h1&gt;
&lt;p&gt;Cool! To sum up the idea above in words, there is some unobserved process $S_t$ guiding the underlying data $E_t$. The Greek letters in the square
box are the corresponding model parameter, which we assume to be fixed for now, and their priors. For example, maybe you own some shares of a company? Then the periodic changes in
your portfolio, $e_t$, will be influenced by the current state of the economy, $s_t$. Hence, you may model this relationship as an HMM.
There are many different variants of the model stated above, which I will discuss in future posts. One may include some autoregressive structure for the observation sequence,
or one may decide to model the state sequence as a higher order Markov chain or even as a semi-Markov chain. Depending on the underlying data you want to model, one may also
want to combine several of these ideas.&lt;/p&gt;
&lt;h1 id=&#34;and-why-are-they-useful&#34;&gt;And why are they useful?&lt;/h1&gt;
&lt;p&gt;It turns out that having an underlying, unobserved process guiding some observed variables is a phenomenon that comes up naturally in many different areas.
While I used an example from finance, there are many areas in genetics, anomaly detection and speech and pattern recognition, among others,
where this structure comes up naturally and SSM can be applied successfully. Moreover, these models&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;can handle structural breaks, shifts, or time-varying parameters of a model. Model parameter will adjust depending on the current state.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;allow you to model complex and nonlinear relationships.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;handle missing and irregular spaced data easily.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;can be used to do forecasting naturally due to their sequential setting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;have interpretable structure to perform inference.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So even if someone is only interested in the observed sequence, the addition of a latent variable offers much additional flexibility that
might not be feasable otherwise. This comes at the price that SSMs are, in general, computationally hard to estimate. I will go further into this topic in a separate post.&lt;/p&gt;
&lt;h1 id=&#34;sampling-our-very-first-state-space-model&#34;&gt;Sampling our very first State Space Model&lt;/h1&gt;
&lt;p&gt;For our first SMM, we will use observations that are normally distributed given the states. In this case, $S_t$ is a first order Markov chain, which can be depicted as a
so called transition matrix $\tau$ . Each row in this matrix has a Categorical distribution, and the parameters thus have to sum up to 1 and are bounded between 0 and 1.
$$
\begin{equation}
\begin{split}
&amp;amp; e_t \sim  Normal(\mu_{s_t}, \sigma_{s_t} ) \\&lt;br&gt;
&amp;amp; s_t \sim  Categorical( \tau_{s_{t-1}}) \&lt;br&gt;
\end{split}
\end{equation}
$$
Let&amp;rsquo;s write down a function that can generate sample paths of the HMM from above. I will mainly use Julia in my blog posts, as this programming language is incredibly fast
and readable, and has some amazing features to make the life of anyone doing scientific computational research much easier. Here are some notes to help
understand the code to sample a single trajectory of said HMM:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The function input are the model distributions stated above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The function output is a single trajectory of the observed and latent variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Before we start the for loop over time, we need to define the initial state. If the latent states of the data are conceived as a subsequence of a
long-running process, the probability of the initial state should be set to the stationary state probabilities
of this unobserved Markov chain. This plays an important part in the estimation paradigm, but for now we simply choose any
of the available states with equal probability. Don&amp;rsquo;t worry if this sounds difficult for you - we will come back to it in a future post.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The for loop samples the new state given the old state, and then the observation given the new state, over time. The corresponding distributions
are stated above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;That&amp;rsquo;s it! Let us have a look:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;
using Plots, Distributions

function sampleHMM(evidence::Vector{&amp;lt;:Distribution}, transition::Vector{&amp;lt;:Distribution}, T::Int64)
        #Initialize states and observations
        state = zeros(Int64, T)
        observation = zeros(Float64, T)

        #Sample initial s from initial distribution
        state[1] = rand( 1:length(transition) ) #not further discussed here
        observation[1] = rand( evidence[ state[1] ] )

        #Loop over Time Index
        for time in 2:T
                state[time] = rand( transition[ state[time-1] ] )
                observation[time] = rand( evidence[ state[time] ] )
        end
        return state, observation
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;sampleHMM (generic function with 1 method)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To round out this post, you can check out this function with different distributions and transition matrices:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;
T = 100
evidence =  [Normal(0., .5), Normal(0.,2.)]
transition = [ Categorical([0.7, 0.3]), Categorical([0.5, 0.5]) ]

state, observation = sampleHMM(evidence, transition, T)

plot( layout=(2,1), label=false, margin=-2Plots.px)
plot!(observation, ylabel=&amp;quot;data&amp;quot;, label=false, subplot=1, color=&amp;quot;gold4&amp;quot;)
plot!(state, yticks = (1:2), ylabel=&amp;quot;state&amp;quot;, xlabel=&amp;quot;time&amp;quot;, label=false, subplot=2, color=&amp;quot;black&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://paschermayr.github.io/files/post/1%20A%20primer%20on%20State%20Space%20Models_2_1.png&#34; &gt;


  &lt;img src=&#34;https://paschermayr.github.io/files/post/1%20A%20primer%20on%20State%20Space%20Models_2_1.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;You can download the full script from my 
&lt;a href=&#34;https://github.com/paschermayr/Shared-Code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub account&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;going-forward&#34;&gt;Going forward&lt;/h1&gt;
&lt;p&gt;We are off to a good start! Next time we will have a closer look at different variants of state space models and their subtle differences.
This should give you a better understanding of possible use cases for SSMs!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
